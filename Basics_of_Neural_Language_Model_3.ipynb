{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BGXi2WF0Hpl"
      },
      "source": [
        "# Short lecture on \"Basics of Neural Language Model\"\n",
        "\n",
        "**Lecturer: Prof. Kosuke Takano, Kanagawa Institute of Technology**\n",
        "\n",
        "This short lecture instructs the basics of neural language model along with simple python codes. The Large Language Model (LLM) such as OpenAI's ChatGPT and Goolge's Gemini are dramatically changing our life and society with their awesome human-like capability, however their mechanism is not so complicated. This lecture aims to focus on basic components to build the LLM and enlighten how they work in a neural network architecture. Student will write small codes of basic functions consisting of neural networks for the natural language processing and deepen the understanding on the principle."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "Day 1:\n",
        "* Basic of neural network\n",
        "* Word embedding\n",
        "* Sequential neural model for Natural Language Processing\n",
        "\n",
        "Day 2:\n",
        "* Sequential neural model for Natural Language Processing (Cont.)\n",
        "* Transformer\n",
        "* Conversation application by GPT"
      ],
      "metadata": {
        "id": "BJN242xWnXJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirement\n",
        "* PC and Internet connection\n",
        "* Google Colaboratory ... Google account is required"
      ],
      "metadata": {
        "id": "FqWwheIJnnKt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WyrJRoHzjQM"
      },
      "source": [
        "## Execution environment\n",
        "\n",
        "Python programs are very version sensitive.Since the execution environment of Colaboratory will be updated at google's discretion, so we need to check it.<br>\n",
        "Python: 3.10.12 (Februrary 27, 2024)<br>\n",
        "TensorFlow: 2.15.0 (Februrary 27, 2024\n",
        "\n",
        "Be sure to specify GPU or TPU as the runtime type."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -V"
      ],
      "metadata": {
        "id": "rOTii9FkJk3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b8fab8-caf1-4637-e6fa-7abf4702da50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5jmSQp1zngl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c946e4d-c532-4288-c6fd-ab4ef9b4258b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part-5"
      ],
      "metadata": {
        "id": "dld0PX_4pETS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural machine translation\n",
        "\n",
        "* Translation function realized using neural network\n",
        "* In 2014, a sequence-to-sequence model using RNN was devised and put into practical use.\n",
        "* Transformer was invented in 2017 and contributes to improving the performance of machine translation."
      ],
      "metadata": {
        "id": "5xYMARaF5KR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seqence to sequence model\n",
        "\n",
        "* For input sequence data, a sequence-to-seqence (seq-to-seq) model outputs it as another sequence data.\n",
        "* Application: Neural translation, text generation, etc.\n",
        "* A squence to sequence model is also called an encode/decode model because it (1) encodes the input series data, and (2) decodes the encoded result to output the series data.\n",
        "* The encoded result is called a semantic vector.\n",
        "* Since the semantic vector has a fixed length, learning becomes difficult as the length of the input sequence data increases.\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1xnshTq3kThH13CRLV1vbEuRmOvGJ5KAC' width='60%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 1. Seqence to sequence model\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "z-QL9qFnqX57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying a sequence to sequence model of RNN for machine translation\n",
        "\n",
        "* Input the text to be translated as series data, and output the translated text as series data.\n",
        " * I like cat. You like dog. → ฉัน ชอบ แมว คุณ ชอบ สุนัข\n",
        " * I like cat. You like dog. → 私は猫が好きです。あなたは犬が好きです。\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1JBOuHVL_NuonIraFS1MtGkhdtJm-4rhO' width='70%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 2. Basic architecture of a sequence to sequence model for machine translation\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "NImfWVfbx9sE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNMvPOBP_H6Q"
      },
      "source": [
        "## Attention\n",
        "* Introduced by Bahdanau, Cho, and Bengio for neural machine translation (2014).\n",
        "* Mechanism to focus on specific features of input data (attention) and emphasize them.\n",
        "* Contributes to improving the performance of sequence-to-sequence models.\n",
        "* Also functions as an important component in Transformers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-attention\n",
        "\n",
        "* Adjust the sequence data to emphasize the elements to be focused on within the same input sequence."
      ],
      "metadata": {
        "id": "EFFEbG8cCe-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code example**"
      ],
      "metadata": {
        "id": "KXz7bj6PUldd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fXwiiks3RLw"
      },
      "outputs": [],
      "source": [
        "!wget http://mattmahoney.net/dc/text8.zip\n",
        "!unzip text8.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTqrq3lG3Zs-"
      },
      "outputs": [],
      "source": [
        "from gensim.models.word2vec import Word2Vec, Text8Corpus\n",
        "\n",
        "sentences = Text8Corpus('text8')\n",
        "model = Word2Vec(sentences, vector_size=100)\n",
        "\n",
        "model.save('model.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k0iJ8Jh3cCK"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec.load('model.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1v0lS-H4Ti-"
      },
      "outputs": [],
      "source": [
        "text = \"I book a room at the hotel.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGbaS7xY8kML"
      },
      "outputs": [],
      "source": [
        "text = text.lower() # lowercase\n",
        "text = text.replace('.', ' .') # separate period\n",
        "words = text.split(' ') # Split words by white space"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we create a self-attention weight matrix"
      ],
      "metadata": {
        "id": "ltuD3LgvtstV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNyDgAY78uyA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Creating a self-attention weight matrix\n",
        "a = np.array([])\n",
        "for w1 in words:\n",
        "  for w2 in words:\n",
        "    try:\n",
        "      score = model.wv.similarity(w1, w2)\n",
        "    except:\n",
        "      score = 0\n",
        "\n",
        "    #print(w1, w2, score)\n",
        "    a = np.append(a, score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we draw a heat map of the self-attention weight"
      ],
      "metadata": {
        "id": "dpEMBCS9tnO4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVUflJoB6fFa"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "length = len(words)\n",
        "\n",
        "attention_matrix = a.reshape(length, length)\n",
        "feature_names = words\n",
        "# Make a heat map of self-attention weights\n",
        "sns.heatmap(attention_matrix, annot=True,\n",
        "            xticklabels=feature_names,\n",
        "            yticklabels=feature_names)\n",
        "\n",
        "# Draw a graph\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 5-1**\n",
        "Draw a heat map of the self-attention weight for the following English sentence.\n",
        "<br><br>\n",
        "Sentence:<br>\n",
        "I cut orages with a knife."
      ],
      "metadata": {
        "id": "2eHJhoY4HOi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention in a sequence to seqence model with RNN\n",
        "* The concatenated outputs of each cell for the input sequence form the sequence of semantic vectors.\n",
        "* When inputting to the decoder cells, considering which part of the context vectors to focus (attend) on, generates the context vector.\n",
        "* Even when the input sequence data is long, the accuracy remains high.\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1Logb1lxDG7YCZ2ndITEHo6AVHtV-OzAZ' width='70%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 3. Attention in a sequence to seqence model with RNN\n",
        "</center>"
      ],
      "metadata": {
        "id": "IyMmiHN2CU8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture of a RNN-based sequence to seqence model with attention\n",
        "Figure 4 shows an architecture of a RNN-based sequence to seqence model with attention, where attention layer is added in the original architecture as shown in Figure 3.In addition, encode outputs a sequence of semantic vectors that is used for the attention calculation to the input sequence at a decorder.\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1E4EdUJluX2Tad0beRfcdtXA3n6qpifXY' width='70%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 4. Architecture of a RNN-based sequence to seqence model with attention\n",
        "</center>"
      ],
      "metadata": {
        "id": "vASNKAomMBeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating context vector in attenion calculation\n",
        "\n",
        "Context vector is created in attenion calculation in the following steps.\n",
        "\n",
        "Step-1: For the output of cell $h'_i$ at the decoder, calculate the inner product with each semantic vector $[\\mathbf{h}_1, \\mathbf{h}_2, \\cdots, \\mathbf{h}_n]$ in the semantic vector sequence, and calculate the weight vector $[a_1 , a_2, \\cdots, a_n]$ is obtained.\n",
        "\n",
        "$$ \\mathbf{a} = [a_1 , a_2, \\cdots, a_n] = [\\mathbf{h}_1, \\mathbf{h}_2, \\cdots, \\mathbf{h}_n] \\cdot \\mathbf{h}'_i \\tag{1}$$\n",
        "\n",
        "Step-2: Normalize $[a_1, a_2, \\cdots, a_n]$ applying softmax so that the sum is 1, and create the normalized weight vector $[a'_1, a'_2, \\cdots , a'_n]$\n",
        "\n",
        "$$ [a'_1, a'_2, \\cdots , a'_n] = softmax([a_1 , a_2, \\cdots, a_n]) \\tag{2}$$\n",
        "\n",
        "Step-3: Calculate the weighted sum of each semantic vector in the semantic vector sequence to obtain the context vector $c_i$.\n",
        "\n",
        "$$ \\mathbf{c}_i = a'_1 \\mathbf{h'}_1 + a'_2 \\mathbf{h'}_2 + \\cdots + a'_m \\mathbf{h'}_m = \\sum^m_{k=1}a'_i \\mathbf{h}'_i \\tag{3}$$\n",
        "<br>\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1TXp0poDkllbu3sFjBzvJxbKfjSThmPyB' width='70%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 5. Creation of context vector in attention layer\n",
        "</center>"
      ],
      "metadata": {
        "id": "B9CBxDkzKod0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code example\n",
        "\n",
        "First, we calculate each output $\\mathbf{h}_j$ of the left encoder in a pseudo manner, and create a sequence of semantic vectors using the example input sentence as follows.\n",
        "<br><br>\n",
        "Sentence:<br>\n",
        "I book a room at the hotel."
      ],
      "metadata": {
        "id": "vF7i7dgaR5md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define RNN_cell0(x, Wx, b) and RNN_cell(x, o, Wx, Wo, b) again."
      ],
      "metadata": {
        "id": "MHFqBKGXa1Cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "wordvec_size = 100\n",
        "hidden_size = 5\n",
        "\n",
        "Wx = np.random.randn(wordvec_size, hidden_size)\n",
        "Wo = np.random.randn(hidden_size, hidden_size)\n",
        "b = np.zeros(hidden_size)"
      ],
      "metadata": {
        "id": "LXJDOWP_Ot0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RNN_cell0(x, Wx, b):\n",
        "\n",
        "  _o = np.dot(x, Wx) + b\n",
        "  o = np.tanh(_o)\n",
        "\n",
        "  return o"
      ],
      "metadata": {
        "id": "s9FgN-z3OoPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RNN_cell(x, o, Wx, Wo, b):\n",
        "\n",
        "  _o = np.dot(o, Wo) + np.dot(x, Wx) + b\n",
        "  o = np.tanh(_o)\n",
        "\n",
        "  return o"
      ],
      "metadata": {
        "id": "35I7X-L1OsyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pre-trained word2vec model."
      ],
      "metadata": {
        "id": "Y78LovycbF23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec.load('model.bin')"
      ],
      "metadata": {
        "id": "_5Vpr-3-bEta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = model.wv[\"i\"]\n",
        "x2 = model.wv[\"book\"]\n",
        "x3 = model.wv[\"a\"]\n",
        "x4 = model.wv[\"room\"]\n",
        "x5 = model.wv[\"at\"]\n",
        "x6 = model.wv[\"the\"]\n",
        "x7 = model.wv[\"hotel\"]"
      ],
      "metadata": {
        "id": "SkjZfEhoPS6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculate semantic vectors."
      ],
      "metadata": {
        "id": "RDU3ciZmsj_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h1 = RNN_cell0(x1, Wx, b)\n",
        "h2 = RNN_cell(x2, h1, Wx, Wo, b)\n",
        "h3 = RNN_cell(x3, h2, Wx, Wo, b)\n",
        "h4 = RNN_cell(x4, h3, Wx, Wo, b)\n",
        "h5 = RNN_cell(x5, h4, Wx, Wo, b)\n",
        "h6 = RNN_cell(x6, h5, Wx, Wo, b)\n",
        "h7 = RNN_cell(x7, h6, Wx, Wo, b)"
      ],
      "metadata": {
        "id": "s9e7DvR2QEqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(h1)\n",
        "print(h2)"
      ],
      "metadata": {
        "id": "qrDTLmnmcRue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we enter the sentence \"Dinner at the restaurant is my favorite.\" into the decoder on the right. At this example, output $\\mathbf{hd}'_1$ of the first cell in a pseudo manner."
      ],
      "metadata": {
        "id": "n_oPZw7cSC7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"Dinner at the restaurant is my favorite.\"\n",
        "\n",
        "y1 = model.wv[\"dinner\"]\n",
        "hd1 = RNN_cell0(y1, Wx, b)"
      ],
      "metadata": {
        "id": "WIP0jWgIQyDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We generate a context vector paying attention on \"dinner\". First, calculate the weights."
      ],
      "metadata": {
        "id": "X4sMjvhrSLey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a1 = np.dot(h1, hd1)\n",
        "a2 = np.dot(h2, hd1)\n",
        "a3 = np.dot(h3, hd1)\n",
        "a4 = np.dot(h4, hd1)\n",
        "a5 = np.dot(h5, hd1)\n",
        "a6 = np.dot(h6, hd1)\n",
        "a7 = np.dot(h7, hd1)"
      ],
      "metadata": {
        "id": "b4ykSNVPRMg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, generate a context vector for \"dinner\" by computing a weighted sum. In this example, normalization by softmax is not applied for the weight values."
      ],
      "metadata": {
        "id": "4-KuSgLgSrL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c1 = a1 * h1 + a2 * h2 + a3 * h3 + a4 * h4 + a5 * h5 + a6 * h6 + a7 * h7"
      ],
      "metadata": {
        "id": "MR4V4jeBS2sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(c1)"
      ],
      "metadata": {
        "id": "XRadDTM-daop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practice 5-2\n",
        "Generate a context vector for paying attention on \"restaurant\". Please use RNN_cell(x, o, Wx, Wo, b) for calculating outputs $\\mathbf{hd}_2$, $\\mathbf{hd}_3$, and so on."
      ],
      "metadata": {
        "id": "Mm6IwtUITRYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"Dinner at the restaurant is my favorite.\"\n",
        "\n",
        "# at\n",
        "y2 = model.wv[\"at\"]\n",
        "hd2 = RNN_cell(y2, hd1, Wx, Wo, b)\n",
        "\n",
        "# the\n",
        "y3 = model.wv[\"the\"]\n",
        "hd3 = RNN_cell(y3, hd2, Wx, Wo, b)\n",
        "\n",
        "# restaurant\n",
        "y4 = model.wv[\"restaurant\"]\n",
        "hd4 = RNN_cell(y4, hd3, Wx, Wo, b)"
      ],
      "metadata": {
        "id": "sHW7tjtxfJKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate attention weight for \"restaurant\"\n",
        "a1 = np.dot(h1, hd4)\n",
        "a2 = np.dot(h2, hd4)\n",
        "a3 = np.dot(h3, hd4)\n",
        "a4 = np.dot(h4, hd4)\n",
        "a5 = np.dot(h5, hd4)\n",
        "a6 = np.dot(h6, hd4)\n",
        "a7 = np.dot(h7, hd4)"
      ],
      "metadata": {
        "id": "e0pbGxmIf973"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate context vector for \"restaurant\"\n",
        "c4 = a1 * h1 + a2 * h2 + a3 * h3 + a4 * h4 + a5 * h5 + a6 * h6 + a7 * h7"
      ],
      "metadata": {
        "id": "t4uMXAqzgDsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part-6"
      ],
      "metadata": {
        "id": "B5Ul9J077coI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s49voOvohoc4"
      },
      "source": [
        "## Transformer\n",
        "\n",
        "* Proposed by Vaswani et al. in 2017\n",
        "* Although it was proposed as a machine translation model, it is also widely used in natural language processing and image processing.\n",
        "* BLEU score of 28.4 with English-German translation\n",
        " * BLEU score: score to evaluate the accuracy of machine translation\n",
        "* Does not have a sequential structure like RNN, and can be accelerated by parallel calculation\n",
        "* Based on deep learning models such as BERT and GPT-n\n",
        "* Vision Transformer (ViT) is an example of application to image processing."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture of Transformer\n",
        "* Multi-head attention: Attention mechanism based on a scaled-dot product calculation that takes query, key, and value as input. A pararell calculation structure for the different contexts is called a multi-head.\n",
        "* Masked multi-head attention: An attention mechanism that prevents the model from referring to subsequent words.\n",
        "* Positional encoding: Embedded information about the position of a word (Vaswani et al. proposed a calculation method using sine and cosine functions)\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1_lq0sXwIOjnzYbm4MZYzg3Tr1V6-xw71' width='50%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 6. Architecture of Transformer (Vaswani, A. et. al, Attention Is All You Need, 2017)\n",
        "</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "aahUXhlGMvpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional encoding\n",
        "* Encoding processing that gives positional information to each word (token) in a sentence\n",
        "\n",
        "$$ PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}}) \\tag{4}$$\n",
        "$$ PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}}) \\tag{5}$$\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1SzCdieqFTyHQjzh-Hk3G0jvCv-fJRU7n' width='40%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 8. Positional encoding\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "OV1tvf7GVHgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code example**\n",
        "\n",
        "Define a positional-encoding function positional_encoding(pos, i, dim)."
      ],
      "metadata": {
        "id": "y-99tOYBfwSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def positional_encoding(pos, i, dim):\n",
        "  if i//2 == 0:\n",
        "    return np.sin(pos/10000**(1/dim))\n",
        "  else:\n",
        "    return np.cos(pos/10000**((i-1)/dim))"
      ],
      "metadata": {
        "id": "h3vA0IUmVGrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate the first (word) positional information in a 100-dimensional vector."
      ],
      "metadata": {
        "id": "zhw_6dC0VgP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 100):\n",
        "  print(positional_encoding(1, i, 100))"
      ],
      "metadata": {
        "id": "f9ruldImVosv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 6-1**\n",
        "Generate the second and third positional information in 100-dimensional vector."
      ],
      "metadata": {
        "id": "vdmQczfGWoYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code example**\n",
        "Store the positional information up to the 30th in an array (30 positions x 100 dimensional vector). Then visualize it."
      ],
      "metadata": {
        "id": "gRqoStonV1cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pe = np.zeros((30, 100))\n",
        "for i in range(0,30):\n",
        "  for j in range(0, 100):\n",
        "    pe[i][j] = positional_encoding(i, j, 100)\n",
        "\n",
        "print(pe)"
      ],
      "metadata": {
        "id": "fEBMnfDMVrvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(pe)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8dIcaTn0WDVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embed positional encoding into each word of \"I book room at the hotel.\""
      ],
      "metadata": {
        "id": "y08t_uwoXYK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xe1 = x1 + pe[0] # I\n",
        "xe2 = x2 + pe[1] # book\n",
        "\n",
        "print(xe1.shape)\n",
        "print(xe2.shape)"
      ],
      "metadata": {
        "id": "Z2YK8ihnXxcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 6-2**\n",
        "Generate vectors with positional encoding embedded for the remaining words \"room\", \"at\", \"the\", and \"hotel\"."
      ],
      "metadata": {
        "id": "6j3wNMizYRRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-head attention\n",
        "\n",
        "* Transform input data in different contexts by linear layers and process in parallel with each scaled dot-product attention\n",
        "* The output is generated by concatinating horizontally the vector outputs of each attention.\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1cpMJTclA31kwMsLZN_Jv19cvXjp8ETEN' width='30%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 7. Multi-head attention\n",
        "</center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wkfVKpgBhKxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaled Dot-Product Attention\n",
        "\n",
        "* Attention mechanism by Query, Key, Value\n",
        "* Normalized (scaled) by the size of the vector\n",
        "* Positional information of words using positional encoding to input word vector\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mcZ6_A-AgDBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ Attention (Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}{})V \\tag{9}$$\n",
        "\n",
        "Step-1: Matrix product of $\\mathbf{Q}$ and $\\mathbf{K}^T$ <br>\n",
        "Step-2: Normalization with $\\sqrt{d_k}$ <br>\n",
        "Step-3: Applying softmax function <br>\n",
        "Step-4: Multiply $\\mathbf{V}$\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1OIk3G99JJNBquU5k8aNTXwkNTSQ4SoFA' width='70%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 6. Scaled dot-product attention\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "XNCmJ0kRBoaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code example\n",
        "\n",
        "For now, to simplify the discussion, we will use randomly generated vectors.Finally, we use word2vec vectors."
      ],
      "metadata": {
        "id": "a3p7qA9q9Tsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "batch_size = 1 # 1 sentence e.g. I like dog\n",
        "seq_length = 3 # 3 words\n",
        "input_dim = 4 # Vector dimensions for word  *word2vec uses 100 dimenseions vector\n",
        "\n",
        "x = np.random.randn(batch_size, seq_length, input_dim)\n",
        "print (x)"
      ],
      "metadata": {
        "id": "T40zbbwT9nvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use pytorch in this example."
      ],
      "metadata": {
        "id": "U5yY1sRl-OUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "7TDFfsOt-R9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert numpy array to pytorch tensor."
      ],
      "metadata": {
        "id": "u_Ga8eD7-Uyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.from_numpy(x.astype(np.float32)).clone()\n",
        "\n",
        "# Check the dimension.\n",
        "d_k = x.size()[-1]\n",
        "print(d_k)"
      ],
      "metadata": {
        "id": "45T37nvH-dTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-1: Matrix product of $\\mathbf{Q}$ and $\\mathbf{K}^T$\n",
        "\n",
        "We calculate matrix product of $\\mathbf{Q}$ and $\\mathbf{K}^T$ in the denominator"
      ],
      "metadata": {
        "id": "7UHY6lWr_OWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q = x\n",
        "k = x\n",
        "v = x"
      ],
      "metadata": {
        "id": "icAwVP5m_VOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(q)\n",
        "print(k)\n",
        "print(v)"
      ],
      "metadata": {
        "id": "snqgjjKq_XIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.transpose(-2, -1)"
      ],
      "metadata": {
        "id": "qiCLHQJp_bHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_logits = torch.matmul(q, k.transpose(-2, -1))"
      ],
      "metadata": {
        "id": "J_mIr2OU_eyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-2: Normalization with $\\sqrt{d_k}$"
      ],
      "metadata": {
        "id": "U_9WQTK-ASQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_logits = attn_logits / math.sqrt(d_k)\n",
        "\n",
        "print(attn_logits)"
      ],
      "metadata": {
        "id": "xvhR8qUYHPqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-3: Applying softmax function"
      ],
      "metadata": {
        "id": "Bkx4PUpZHY38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights = F.softmax(attn_logits, dim=-1)\n",
        "\n",
        "print(attention_weights)"
      ],
      "metadata": {
        "id": "uuF_DEn-HYb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-4: Multiply $\\mathbf{V}$\n",
        "\n",
        "The resulting values ​​are the context vector by attention."
      ],
      "metadata": {
        "id": "YWNsLAA6HhZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vectors = torch.matmul(attention_weights, v)\n",
        "\n",
        "print(context_vectors)"
      ],
      "metadata": {
        "id": "8_wmbwDWHglO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 6-3**\n",
        "Make a function of attention(q, k, v) that returns context vectors and attention weights."
      ],
      "metadata": {
        "id": "dhWkCxuhH_Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(q, k, v):\n",
        "  d_k = q.size()[-1]\n",
        "  attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "  attn_logits = attn_logits / math.sqrt(d_k)\n",
        "  attention_weights = F.softmax(attn_logits, dim=-1)\n",
        "  context_vectors = torch.matmul(attention_weights, v)\n",
        "\n",
        "  return context_vectors, attention_weights"
      ],
      "metadata": {
        "id": "qcvxYYEzIo6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values, att = attention (q, k, v)\n",
        "\n",
        "print(values)\n",
        "print(att)"
      ],
      "metadata": {
        "id": "jhMmNicCIs7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code example\n",
        "The linear layer play a roll of projection function. That means the linear layer output the projected vectors for the input vectors, $\\mathbf{q}, \\mathbf{k}, \\mathbf{v}$.\n",
        "\n",
        "Create projection spaces (q_proj, k_proj, v_proj) for each of q, k, v. Here, the number of dimensions of the projection space is embed_dim = 4."
      ],
      "metadata": {
        "id": "BblpEzWjI6TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 4\n",
        "\n",
        "q_proj = nn.Linear(input_dim, embed_dim)"
      ],
      "metadata": {
        "id": "30ruPmVhJwta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = q_proj(x)\n",
        "print (q)"
      ],
      "metadata": {
        "id": "PO9TcSDxJzoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 6-4**\n",
        "\n",
        "1. Similary, make functions k_proj(k) and v_proj(v).\n",
        "2. Then get the projected vectors for k and v."
      ],
      "metadata": {
        "id": "rl3x4tRQMBy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_proj = nn.Linear(input_dim, embed_dim)\n",
        "v_proj = nn.Linear(input_dim, embed_dim)\n",
        "\n",
        "k = k_proj(x)\n",
        "print (k)\n",
        "\n",
        "v = v_proj(x)\n",
        "print (v)"
      ],
      "metadata": {
        "id": "QAg4o2P8J1Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code example\n",
        "\n",
        "Let's use word vectors obtained by word2vec."
      ],
      "metadata": {
        "id": "6bysbBR1Lxek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = model.wv['i']\n",
        "x2 = model.wv['like']\n",
        "x3 = model.wv['dog']\n",
        "\n",
        "print(x1)\n",
        "print(x2)\n",
        "print(x3)"
      ],
      "metadata": {
        "id": "Yyw0wUF5L6WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embed a vector of positional encoding before projection."
      ],
      "metadata": {
        "id": "qhbQx1CZOGkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1_pe = x1 + pe[0] # I\n",
        "x2_pe = x2 + pe[1] # like\n",
        "x3_pe = x3 + pe[2] # dog"
      ],
      "metadata": {
        "id": "O6zbdUlLN8id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_pe = np.array([x1_pe, x2_pe, x3_pe])\n",
        "\n",
        "x_pe = torch.from_numpy(x_pe.astype(np.float32)).clone()\n",
        "print(x_pe)"
      ],
      "metadata": {
        "id": "67FCZz2KL8MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 100\n",
        "embed_dim = 30\n",
        "\n",
        "q_proj = nn.Linear(input_dim, embed_dim)\n",
        "k_proj = nn.Linear(input_dim, embed_dim)\n",
        "v_proj = nn.Linear(input_dim, embed_dim)"
      ],
      "metadata": {
        "id": "08ZLIcAINZjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = q_proj(x_pe)\n",
        "k = k_proj(x_pe)\n",
        "v = v_proj(x_pe)"
      ],
      "metadata": {
        "id": "3tto8UExNcN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vectors, atttention_weights = attention (q, k, v)\n",
        "\n",
        "print(context_vectors, atttention_weights)"
      ],
      "metadata": {
        "id": "uyl0XFvwNd4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying Transformer encoder for sentiment analysis\n",
        "\n",
        "Transformer encoder alone can be applied for NLP tasks such as sentiment analysis, document classification."
      ],
      "metadata": {
        "id": "r2HfScUYN6IM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code example**\n",
        "\n",
        "Let's classify movie review texts in the IMDB dataset using the Transformer encoder."
      ],
      "metadata": {
        "id": "psgcywKAgJfi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHtrgk4khoBe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DRm4E2Whufo"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000 # Number of words\n",
        "embed_dim = 256 # Dimension of embedding\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-nTFpZwnLwm"
      },
      "outputs": [],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkicmeUYncJM"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rv2efUOInXUI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in int_train_ds:\n",
        "  print(item)"
      ],
      "metadata": {
        "id": "BZcDANRrv-YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi0b9CcYkNDA"
      },
      "outputs": [],
      "source": [
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "i1DFeFymapZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 6-5**\n",
        "* Increase the number of epochs to 10 in the Transformer encoder and check if the classification accuracy is improved. (If it takes longer to execute, you can reduce the number of epochs.)\n",
        "* In addition to SimpleRNN, LSTM, and GRU, which we checked last time, compare and discuss the classification accuracy of four models including Transformer encoder. Furthermore, let's compare by also focusing on the number of model parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "q2UJKn_ALzUZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMK1SPj4zebm"
      },
      "source": [
        "## Reference\n",
        "* Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/abs/1810.04805v1, 2018.\n",
        "* Keras official Website, https://keras.io/examples/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}