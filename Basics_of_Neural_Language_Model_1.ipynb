{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9aIsWInjUrU2",
        "hiV4yWRxbBtk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Short lecture on \"Basics of Neural Language Model\"\n",
        "\n",
        "**Lecturer: Prof. Kosuke Takano, Kanagawa Institute of Technology**\n",
        "\n",
        "This short lecture instructs the basics of neural language model along with simple python codes. The Large Language Model (LLM) such as OpenAI's ChatGPT and Goolge's Gemini are dramatically changing our life and society with their awesome human-like capability, however their mechanism is not so complicated. This lecture aims to focus on basic components to build the LLM and enlighten how they work in a neural network architecture. Student will write small codes of basic functions consisting of neural networks for the natural language processing and deepen the understanding on the principle."
      ],
      "metadata": {
        "id": "vG8ghNIu0z4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "Day 1:\n",
        "* Basic of neural network\n",
        "* Word embedding\n",
        "* Sequential neural model for Natural Language Processing\n",
        "\n",
        "Day 2:\n",
        "* Sequential neural model for Natural Language Processing (Cont.)\n",
        "* Transformer\n",
        "* Conversation application by GPT"
      ],
      "metadata": {
        "id": "OieQZOCISYzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirement\n",
        "\n",
        "* PC and Internet connection\n",
        "* Google Colaboratory  ... Google account is required\n"
      ],
      "metadata": {
        "id": "ZqTiXjBsTsfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution environment\n",
        "\n",
        "Python programs are very version sensitive.Since the execution environment of Colaboratory will be updated at google's discretion, so we need to check it.<br>\n",
        "Python: 3.10.12 (Februrary 27, 2024)\n",
        "\n",
        "Be sure to specify GPU or TPU as the runtime type."
      ],
      "metadata": {
        "id": "oCsUej-OG9vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2EtscWwG6WF",
        "outputId": "a27e0098-2787-49b6-b9c4-73cd1d6c41c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJOyLI1m_2PI"
      },
      "source": [
        "# Part-1\n",
        "\n",
        "## 1. Principle of neural network\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction of neural network\n",
        "\n",
        "Neural networks are mathematical models that represent nerve cells (neurons) in the brain and their connections as a network of artificial neurons.\n",
        "The first artificial neuron was devised in 1943 by Dr. W. McCulloch as a formal neuron. Also, in 1957, Dr. F. Rosenblatt devised a perceptron that applied formal neurons. Among perceptrons, those with two network layers are called simple perceptrons, and those with three or more layers are called multilayer perceptrons (Figure 1). The multilayer perceptron was repeatedly improved, and in the 1980s, error backpropagation, a method for efficiently learning neural networks, was applied, resulting in a multilayer perceptron consisting of several layers. It has become possible to realize so-called \"shallow\" neural networks.\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1WAX0TCABY4V-qTHPjTeFrwZtrW_W7l61' width='80%'>\n",
        "</center>\n",
        "\n",
        "<center>Figure 1. Initial perseptron</center>"
      ],
      "metadata": {
        "id": "GaWpcn_OMi4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Furthermore, in the late 2000s, (1)problems such as the vanishing gradient problem, which is a phenomenon in which the learning efficiency of error backpropagation decreases as the network layer becomes deeper, were solved, and (2)the computational performance of computers improved, making it possible to reduce the learning time of neural networks. As a result, it has become possible to realize deep neural networks (Figure 2) with deeper network layers.\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=13x2zKSy1HHKz5ReZcUPMddO-aIq-sZpi' width='70%'>\n",
        "</center>\n",
        "\n",
        "<center>Figure 2. Deep neural network</center>"
      ],
      "metadata": {
        "id": "RozhMxCAZNb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principle of perceptron\n",
        "\n",
        "As shown in Figure 1, the simple perceptron accepts n values ​​as input and outputs one value. This calculation is done in two stages. First, the n input values ​​are multiplied by their corresponding weight values, and the sum is calculated. We will call this a \"weighted sum.\" Next, the weighted sum is input to a transformation function called an \"activation function\", and an output value is calculated depending on the properties of the activation function. Step functions, sigmoid functions, ReLU functions, etc. are often used as activation functions.\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=157Bff_Wen0w72_VDbmLvdG7xAEBPG5k-' width='50%'>\n",
        "</center>\n",
        "\n",
        "<center>Figure 3. Overview of calculations in perceptron</center>"
      ],
      "metadata": {
        "id": "ZQ6B_Uopeibb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 3 shows an overview of the calculation in the perceptron. In Figure 3, if $n$ input values ​​are $x_1$, $x_2$, …, $x_n$, the corresponding weights are $w_1$, $w_2$, …, $w_n$, and the bias is b, the weighted sum $\\_y$ can be calculated as follows.\n",
        "\n",
        "$$\\_y=\\sum_{k=1}^n x_k\\cdot w_k + b \\tag{1}$$"
      ],
      "metadata": {
        "id": "20AWsczPPYJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of two input values $x_1$ and $x_2$, the formula is as follows.\n",
        "\n",
        "$$\\_y=\\sum_{k=1}^n x_k\\cdot w_k + b = x_1\\cdot w_1 + x_2\\cdot w_2 + b \\tag{2}$$"
      ],
      "metadata": {
        "id": "YeaIKQ1BUhOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, suppose the activation function be Activate(), the output $y$ of the perceptron with the weighted sum $\\_y$ as input is expressed by the following equation.\n",
        "\n",
        "$$y = Activate(\\_y) = Activate(\\sum_{k=1}^n x_k\\cdot w_k + b) \\tag{3}$$"
      ],
      "metadata": {
        "id": "CUNk0XLlVBLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Python basic 1**\n",
        "\n",
        "Using python, calculate the following formula.\n",
        "\n",
        "(1) 1 + 2<br>\n",
        "(2) 4 - 1 + 2<br>\n",
        "(3) (4 + 5) x 4<br>\n",
        "(4) 5 / (1 + 2 + 3)\n"
      ],
      "metadata": {
        "id": "PCJX5YxksNoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 1 + 2\n",
        "b = 4 - 1 + 2\n",
        "c = (4 + 5) * 4\n",
        "d = 5 / (1 + 2 + 3)\n",
        "\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "print(d)"
      ],
      "metadata": {
        "id": "E08buUOMsJRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Python basic 2**\n",
        "\n",
        "Using python, calculate the following formula when p = 1, q = 2.\n",
        "\n",
        "(1) p + q<br>\n",
        "(2) p - q<br>\n",
        "(3) p $\\times$ q<br>\n",
        "(4) p / q"
      ],
      "metadata": {
        "id": "gSPct75Gtmhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = 1\n",
        "q = 2\n",
        "\n",
        "a = p + q\n",
        "b = p - q\n",
        "c = p * q\n",
        "d = p / q\n",
        "\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "print(d)"
      ],
      "metadata": {
        "id": "WcCH7wpstSqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Python basic 3**\n",
        "Definfe a function add(x, y) for adding two input values x and y. Furthermore, calculate the result when x = 2 and y = 3."
      ],
      "metadata": {
        "id": "7atlDyAnu-a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add(x, y):\n",
        "  z = x + y\n",
        "  return z"
      ],
      "metadata": {
        "id": "HNhXVrgDu4p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x =2\n",
        "y =3\n",
        "\n",
        "z = add(x, y)\n",
        "\n",
        "print(z)"
      ],
      "metadata": {
        "id": "p8RQEHhtvh9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code example**\n",
        "\n",
        "We can write Equation (2) in python code as follow."
      ],
      "metadata": {
        "id": "7e14BtSaV_OV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_y = x1 * w1 + x2 * w2 + b"
      ],
      "metadata": {
        "id": "R0xwZIHX-GBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate using values, (x1,x2)=(2,4),(w1,w2)=(3,5), b=1."
      ],
      "metadata": {
        "id": "YHtKgEiB-Hs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x1, x2) = (2, 4)\n",
        "(w1, w2) = (3, 5)\n",
        "b = 1\n",
        "\n",
        "_y = x1 * w1 + x2 * w2 + b\n",
        "\n",
        "print(_y)"
      ],
      "metadata": {
        "id": "xzw-4Gwr-KcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 1-1**\n",
        "1. Write a python program that calculates the weighted sum for two input values ​​$x_1$ and $x_2$ as a function _two_input_weight(). As arguments, take two input values ​​x1, x2, two weight values ​​w1, w2, and bias b, and output _y as the return value as follows.<br><br>\n",
        "_y = _two_input_weight(x1, x2, w1, w2, b)\n",
        "\n",
        "2. Furthermore, calculate results when $(x_1, x_2) = (2, 4), (w_1, w_2) = (3, 5)$, and $b = 1$."
      ],
      "metadata": {
        "id": "z-mxuXfNXaRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def _two_input_weight(x1, x2, w1, w2, b):\n",
        "\n",
        "  _y = x1 * w1 + x2 * w2 + b\n",
        "  return _y"
      ],
      "metadata": {
        "id": "sw6rJFAOUWXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x1, x2) = (2, 4)\n",
        "(w1, w2) = (3, 5)\n",
        "b = 1\n",
        "\n",
        "_y = _two_input_weight(x1, x2, w1, w2, b)\n",
        "\n",
        "print (_y)"
      ],
      "metadata": {
        "id": "9gdAPanWYoOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer expression of neural network\n",
        "\n",
        "A neural network is composed of a large number of neurons. However, if each adjacent network is represented as a single network layer as shown in Figure 4, it can be seen that the output of neurons is propagated sequentially from network layer to network layer (Figure 5).As shown in Figure 4, we call a layer connects all neurons between adjacent network layers and calculates a weighted sum and activation \"fully connected layer\".\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1dGcNIotcQJuHzFpeQ6l6zVI2hd3r89Wc' width='30%'>\n",
        "</center>\n",
        "\n",
        "<center>Figure 4. Fully connected layer</center>\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1F0CMnq3eF91nGk8qFf7vH4s7jxcd1Ua3' width='30%'>\n",
        "</center>\n",
        "\n",
        "<center>Figure 5. Layer expression of neural network </center>"
      ],
      "metadata": {
        "id": "EuCPjZ1Ifshs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculation for extending to fully connted layers\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1vtzsNTep-RtQF0f0sN21L-nnTAhoR24q' width='70%'>\n",
        "</center>\n",
        "\n",
        "<center>Figure 6. Calculation flow of weighted sums in fully connected layers </center>\n",
        "\n",
        "Figure 6 shows the flow of calculating a weighted sum in a fully connected layer consisting of two neurons for two input values. In Figure 6, if the two input values ​​are x1, x2, the corresponding weights are (w11, w21), (w12, w22), and the biases are b1, b2, then the weighted sums _y1, _y2 are as follows. It can be calculated as follows.\n",
        "\n",
        "$$\\_y_1=x_1\\cdot w_{11}+x_2\\cdot w_{12}+b_1 \\tag{4}$$\n",
        "$$\\_y_2=x_1\\cdot w_{21}+x_2\\cdot w_{22}+b_2 \\tag{5}$$\n",
        "\n",
        "Equations (4) and (5) are vector $\\mathbf{x} = (x_1, x_2)$, matrix $\\mathbf{W} = ((w_{11}, w_{21})$, $(w_{12}, w_{22}))$, vector $\\mathbf{b} = (b_1, b_2)$, vector $\\mathbf{\\_y} = ( \\_y_1, \\_y_2)$, it can be calculated with one formula as shown below.\n",
        "Equation (6) can be used to calculate the weighted sum in a fully connected layer not only for two neurons for two input values ​​but also for $m$ neurons for $n$ input values.\n",
        "\n",
        "$$\\_\\mathbf{y}=\\mathbf{xW}+\\mathbf{b} \\tag{6}$$"
      ],
      "metadata": {
        "id": "I1BF9-lmkuDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Python basic 4**\n",
        "Using python and its numpy library, calculate (1)addition, (2)subtraction, and (3)inner product when (x1,x2)=(2,4),(w1,w2)=(3,5) , and  b=1 ."
      ],
      "metadata": {
        "id": "wioM25H2lKNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.array([5, 2, 3])\n",
        "b = np.array([-1, 0, 1])\n",
        "\n",
        "addition = a + b\n",
        "subtraction = a - b\n",
        "innerprod1 = np.dot(a, b) # inner product\n",
        "innerprod2 = np.matmul(a, b) # inner product\n",
        "\n",
        "print(addition)\n",
        "print(subtraction)\n",
        "print(innerprod1)\n",
        "print(innerprod2)"
      ],
      "metadata": {
        "id": "20y8rHHNpkdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Python basic 5**\n",
        "Using python and numpy library, calculate (1)addition, (2)matrix element product, and (3)matrix product when A ＝[[5, 2], [1, 4]], B = [[-1, 0], [2, 3]]"
      ],
      "metadata": {
        "id": "w66JBNXowWie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[5, 2], [1, 4]])\n",
        "B = np.array([[-1, 0], [2, 3]])\n",
        "\n",
        "\n",
        "g = A + B\n",
        "h = A * B\n",
        "i = np.matmul(A, B)\n",
        "\n",
        "print(g)\n",
        "print(h)\n",
        "print(i)"
      ],
      "metadata": {
        "id": "VhpXImbcwMtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code example**\n",
        "\n",
        "We can write Equation (5) in python code as follow."
      ],
      "metadata": {
        "id": "mJ9wJ3WMUZVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_y = np.matmul(x, W) + b"
      ],
      "metadata": {
        "id": "_63d08CcUa5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate for vectors and matrices, x=(x1,x2)=(2,4) , W=((w11,w21),(w12,w22))=((3,5),(6,5)), b=(b1,b2)=(1,3). We can use numpy's array function array() for arrays."
      ],
      "metadata": {
        "id": "OQC0ku4HUdJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([2,4])\n",
        "W = np.array([[3, 5],[6,5]])\n",
        "b = np.array([1,3])\n",
        "\n",
        "_y = np.matmul(x, W) + b\n",
        "\n",
        "print(_y)"
      ],
      "metadata": {
        "id": "vVPYrGHcUffJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 1-2**\n",
        "\n",
        " 1. Write a python program that calculates the weighted sum of two neurons for $n$ input values $​​x = (x_1, x_2, \\cdots , x_n)$ as a function _fc_weight() This function calculates the weighted sum of fully connected layers. As arguments, take an array for vector $\\mathbf{x}$ corresponding to $n$ input values, a matrix $\\mathbf{W}$ corresponding to each weight value of $m$ neurons, and an array for vector $\\mathbf{b}$ corresponding to each bias, and output an array for vector $\\_\\mathbf{y}$ as a return value.\n",
        "<br><br>\n",
        "_y = _fc_weight(x, W, b)\n",
        "\n",
        "2. Furthermore, calculates the fesults for $\\mathbf{x} = (x_1, x_2) = (2, 4)$, $\\mathbf{W} = ((w_{11}, w_{21}), (w_{12}, w_{22})) = ((3, 5), (6, 5))$, $\\mathbf{b} = (b_1, b_2) = (1, 3)$. Use numpy's array function array() for arrays."
      ],
      "metadata": {
        "id": "NoqDmPkYzlrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def fc_weight(x, W, b):\n",
        "  _y = np.matmul(x, W) + b\n",
        "\n",
        "  return _y"
      ],
      "metadata": {
        "id": "WMe3uC4_06MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Furthermore, calculates the fesults for $\\mathbf{x} = (x_1, x_2) = (2, 4)$, $\\mathbf{W} = ((w_{11}, w_{21}), (w_{12}, w_{22})) = ((3, 5), (6, 5))$, $\\mathbf{b} = (b_1, b_2) = (1, 3)$. Use numpy's array function array() for arrays."
      ],
      "metadata": {
        "id": "gqQpe9-C2iSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([2,4])\n",
        "W = np.array([[3, 5],[6,5]])\n",
        "b = np.array([1,3])\n",
        "\n",
        "_y = fc_weight(x, W, b)\n",
        "\n",
        "print(_y)"
      ],
      "metadata": {
        "id": "wU1fODbh3AQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying activation function\n",
        "\n",
        "A sigmoid function is any mathematical function whose graph has a characteristic S-shaped curve. The sigmoid function is applyed as the activate function in Figure 3.\n",
        "\n",
        "$$ S(x) = \\frac{1}{1+e^{-x}} \\tag{7}$$\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1pZn0O6YanQoQVDrPRxWhZy7pGzs85pYv' width='40%'>\n",
        "</center>\n",
        "\n",
        "<center>Figure 7. Graph of sigmoid function </center>"
      ],
      "metadata": {
        "id": "W0JAvMNU5Lr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code example**\n",
        "\n",
        "Let's define a sigmoid funtion sigmoid(x) using python."
      ],
      "metadata": {
        "id": "HaErYH3H9jXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "8lY9MMYa9ryt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Python basic 6**\n",
        "Draw a graph of sigmoid function using matplotlib library."
      ],
      "metadata": {
        "id": "3UL-1-UL9wAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "graph = fig.add_subplot(111)\n",
        "graph.grid(linestyle=\"dotted\")\n",
        "graph.set_xlabel(\"x\")\n",
        "graph.set_ylabel(\"y\")\n",
        "\n",
        "x = np.linspace(-10, 10, num=300)\n",
        "\n",
        "y = sigmoid(x)\n",
        "graph.plot(x, y)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MyB_KC1q7pfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final output of perceptron\n",
        "As shown in the formula (8), the output of perceptron is calculated by applying the activation functin for the weighed sum.Especially, when applying a sigmoid function, the formula (8) is described as the formula (9).\n",
        "\n",
        "$$y = Activate(\\_y) = Activate(\\mathbf{xW}+\\mathbf{b}) \\tag{8}$$\n",
        "$$y = Activate(\\_y) = sigmoid(\\mathbf{xW}+\\mathbf{b}) \\tag{9}$$"
      ],
      "metadata": {
        "id": "-AYKH4gdBF-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code example**\n",
        "\n",
        "Let's define a function of calculation in fully connected layer fc_layer(). Use a sigmoid function as the activation function. As arguments, take an array for vector $\\mathbf{x}$ corresponding to $n$ input values, a matrix $\\mathbf{W}$ corresponding to each weight value of $m$ neurons, and an array for vector $\\mathbf{b}$ corresponding to each bias, and output an array for vector $\\mathbf{y}$ as a return value."
      ],
      "metadata": {
        "id": "hOWs6c13C3Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fc_layer(x, W, b):\n",
        "  _y = fc_weight(x, W, b)\n",
        "  y = sigmoid(x)\n",
        "\n",
        "  return y"
      ],
      "metadata": {
        "id": "VOIwFl6aEyG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Furthermore, calculates the fesults for x=(x1,x2)=(2,4), W=((w11,w21),(w12,w22))=((3,5),(6,5)),  b=(b1,b2)=(1,3)."
      ],
      "metadata": {
        "id": "31yyCUTBFNOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([2,4])\n",
        "W = np.array([[3, 5],[6,5]])\n",
        "b = np.array([1,3])\n",
        "\n",
        "y = fc_layer(x, W, b)\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "id": "_kC2QlLvFZpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part-2"
      ],
      "metadata": {
        "id": "jznZTyqFKj-l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FRXS85qAZmY"
      },
      "source": [
        "## What is natural language processing?\n",
        "\n",
        "* Languages ​​used in everyday conversation, such as Japanese, Thai, and English are called natural languages.\n",
        "Natural language processing is a process that enables computers to perform machine translation, automatic summarization, text classification, context understanding, conversation generation, etc. through processes such as word and phrase extraction and dependency analysis in natural language.\n",
        "\n",
        "* Application examples of natural language processing\n",
        " * Document classification\n",
        " * Search\n",
        " * Machine translation\n",
        " * Document summary\n",
        " * Question answer\n",
        " * Dialogue\n",
        " * Part-of-speech tagging\n",
        " * Word splitting\n",
        " * Semantic disambiguation\n",
        " * Named entity extraction\n",
        " * Parsing\n",
        " * Predicate term structure recognition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aIsWInjUrU2"
      },
      "source": [
        "## Natural language processing and deep learning\n",
        "* Estimation of the probability of word appearance in a document using topic models and part-of-speech estimation using hidden Markov models have been performed.\n",
        "\n",
        "* In 2013, a method such as word2vec that uses neural networks to learn distributed representations of words was devised, and RNN and LSTM have been applied to natural language processing, as well as machine translation, dialogue generation, automatic summarization, and image description. Applications such as generation have expanded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiV4yWRxbBtk"
      },
      "source": [
        "## Flow of natural language processing\n",
        "\n",
        "1. Dataset preparation\n",
        "2. Pre-processing\n",
        "3. Quantifying words\n",
        "4. Learning dataset = building applied model\n",
        "5. Classification and regression using applied models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iib5GxyjHmAQ"
      },
      "source": [
        "## Pre-processing\n",
        "* Processing the text into a format that is easy for analysis programs to process by processing such as n-gram division and stop words.\n",
        "\n",
        " * Unified notation\n",
        " * Lowercase and uppercase\n",
        " * Word replacement\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code example**"
      ],
      "metadata": {
        "id": "F9HOVkuQXowj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BQqy1otqT41"
      },
      "source": [
        "text = 'I look at sky and you look in mirror.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "id": "EFisl1ISc4Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6R6ykJgmKSj"
      },
      "source": [
        "text = text.lower() # lowercase\n",
        "text = text.replace('.', ' .') # separate period\n",
        "words = text.split(' ') # Split words by white space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVbMTEs7311x"
      },
      "source": [
        "print (words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RovHcmNZeK6V"
      },
      "source": [
        "## Quantifying words\n",
        "* Converts words into number values so that relationships between words can be calculated quantitatively.\n",
        "* The basic way of quantifying words is indexing words and using an index number as a word id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwn6uLqWlsK7"
      },
      "source": [
        "def word2id(words):\n",
        "\n",
        "  word_to_id = {}\n",
        "\n",
        "  for word in words:\n",
        "    if word not in word_to_id:\n",
        "      new_id = len(word_to_id)\n",
        "      word_to_id[word] = new_id\n",
        "\n",
        "  return word_to_id\n",
        "\n",
        "def id2word(word_to_id):\n",
        "  id_to_word = {}\n",
        "  for word, id in word_to_id.items():\n",
        "    id_to_word[id] = word\n",
        "\n",
        "  return id_to_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 2-1**\n",
        "For the following sentences, use word2id() to convert words to IDs. Also, let's use id2word() to extract the corresponding word from the id.<br><br>\n",
        "\n",
        "\n",
        "In the mirror, you saw a bird flying across the blue sky."
      ],
      "metadata": {
        "id": "Z6EczzQV8XZA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b5HAFR4nyKF"
      },
      "source": [
        "# Indexing words\n",
        "word_to_id = word2id(words)\n",
        "print(word_to_id)\n",
        "\n",
        "# Reverse lookup from word id\n",
        "id_to_word = id2word(word_to_id)\n",
        "print(id_to_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_to_id['look'])\n",
        "print(id_to_word[7])"
      ],
      "metadata": {
        "id": "FGulQgh_fCJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR9D50p0I1dw"
      },
      "source": [
        "## One-hot vector representation\n",
        "* Another way for quantifiyg words is using one-hot vector representation\n",
        "* One-hot vector is a vector whose element values ​​are 0 and 1, and where only one element is 1. Neural networks that process natural language often use one-hot vectors as input.\n",
        "\n",
        "* For the sentence 'I look at sky and you look in mirror.', when decomposed as in the example above, using the word id, a one-hot vector for each word can be generated as shown below.\n",
        "\n",
        " * i: [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        " * look: [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        " * at: [0, 0, 1, 0, 0, 0, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code example**"
      ],
      "metadata": {
        "id": "PWdtE-TUX2U1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pczPYaF5k_6m"
      },
      "source": [
        "# Convert word-number list data to one-hot format\n",
        "def make_one_hot(corpus):\n",
        "    N = corpus.shape[0]\n",
        "    dim =len(word_to_id)\n",
        "\n",
        "    one_hot = np.zeros((N, dim), dtype=np.int32)\n",
        "    for idx, word_id in enumerate(corpus):\n",
        "        one_hot[idx, word_id] = 1\n",
        "\n",
        "    return one_hot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfnimEn_TfZ9"
      },
      "source": [
        "import numpy as np\n",
        "print(words)\n",
        "corpus = [word_to_id[word] for word in words]\n",
        "corpus = np.array(corpus)\n",
        "\n",
        "print(corpus)\n",
        "\n",
        "make_one_hot(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 2-2**\n",
        "Let's convert the following sentence into a one-hot vector representation.<br><br>\n",
        "\n",
        "In the mirror, you saw a bird flying across the blue sky."
      ],
      "metadata": {
        "id": "P6mhwVKf9RmV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QANFdYqQW4ri"
      },
      "source": [
        "## Language models and context\n",
        "* The process by which words appear in a document is regarded as a stochastic process, and a model that calculates the probability that a word will appear in a certain position is called a language model.\n",
        "* In a language model, the surrounding words used to calculate the probability of a word appearing are called context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WILtPxQYYPHF"
      },
      "source": [
        "## Distributed representation of words (Word embedding)\n",
        "* A word expressed as a vector.\n",
        "* The following methods use neural networks to obtain word distributed representations.\n",
        "\n",
        " * Word2Vec\n",
        " * GloVe\n",
        " * fastText\n",
        "\n",
        "* It can also be trained using regular deep learning to obtain word distributed representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQniugEjdyrH"
      },
      "source": [
        "## Word2Vec\n",
        "* Method for generating distributed representations of words (word embeddings)\n",
        "\n",
        "* Applys CBOW model and skip gram model\n",
        "* Invented by Tomas Mikolov et al. in 2013\n",
        "* Learn the meaning representation of words based on the distribution hypothesis (the hypothesis that the meaning of a word is formed by surrounding words).\n",
        "*Learn with a two-layer neural network. The weights of the hidden layer become the distributed representation of the word.\n",
        "\n",
        "* The distributed representation of the generated words is a vector matrix that represents the meaning of each word, and the distance between words can be calculated.\n",
        "\n",
        " * vector('Paris') - vector('France') + vector('Italy') = vector('Rome') <br>\n",
        " * vector('king') - vector('man') + vector('woman') = vector('queen')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CBOW model\n",
        "\n",
        "* Predict a single word using multiple words as a context.\n",
        "* The order of the context words does not matter.\n",
        "<br>\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=17xtbUuEkH9Vot5HhNX2A8tJWXdWzKbCH' width='70%'>\n",
        "</center>\n",
        "\n",
        "<center>Figure 9. CBOW model </center>\n"
      ],
      "metadata": {
        "id": "aSieG5DxYpmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Skip-gram model\n",
        "\n",
        "* Predict multiple words using one word as context.\n",
        "* Context words are weighted according to its positional proximity to input words.\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1AwKBI_Vqz5s2QsijFt4I3GSUN0znhly8' width='70%'>\n",
        "</center>\n",
        "\n",
        "<center>Figure 10. Skip-gram model </center>\n"
      ],
      "metadata": {
        "id": "QdB3oKxbYz3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 2-3**\n",
        "Let's explain how traing sentences forcreating word distributed representation matrix (embedding matrix) for CBOW model and skip-gram model in Word2Vec, respectively."
      ],
      "metadata": {
        "id": "0O9Wml3-IzUH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwVwumo2ayxr"
      },
      "source": [
        "## Creating a word embedding matrix\n",
        "In order to create a word embedding matrix, a large set of sentences is required. Then, a neural model for extracting embedding matirx can be trained using the corpus by applying word2vec, GLoVe, and so on.\n",
        "\n",
        "Step-1: Get a corpus that includes a set of sentences.<br>\n",
        "Step-2: Taining a neural model using the corpus by applying a embedding creation method such as word2vec and GLoVe."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code example\n",
        "\n",
        "Let'S create a word embedding matrix using the corpus provided at the website below.Here, we use word2vec, which is provided as a gensim library.\n",
        "\n",
        "(Website) http://mattmahoney.net/dc/textdata.html"
      ],
      "metadata": {
        "id": "kApu0qL3dnMh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwJauWPr6x8n"
      },
      "source": [
        "!wget http://mattmahoney.net/dc/text8.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM2rcdUSaHG4"
      },
      "source": [
        "!unzip text8.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKVoWs-bbant"
      },
      "source": [
        "import logging\n",
        "from gensim.models.word2vec import Word2Vec, Text8Corpus\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "sentences = Text8Corpus('text8')\n",
        "model = Word2Vec(sentences, vector_size=100)\n",
        "\n",
        "model.save('model.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT5c1-GibtnA"
      },
      "source": [
        "model = Word2Vec.load('model.bin')\n",
        "\n",
        "# embedding vector of 'dog'\n",
        "print(model.wv['dog'])\n",
        "print(model.wv['dog'].shape)\n",
        "# extract words with similar meaning to 'car'\n",
        "model.wv.most_similar(['car'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check whether the following formula used in the above explanation holds true.\n",
        "\n",
        "vector('Paris') - vector('France') + vector('Italy’) = vector(‘Rome’)\n"
      ],
      "metadata": {
        "id": "pmvqIEnU99yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = model.wv['paris'] - model.wv['france'] + model.wv['italy']\n",
        "\n",
        "model.wv.most_similar(vector)"
      ],
      "metadata": {
        "id": "Ft2kzadN-R1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 2-4**\n",
        "Let's try to see if the following formula holds true.\n",
        "\n",
        "vector('king') - vector('man') + vector('woman') = vector('queen')"
      ],
      "metadata": {
        "id": "2U_A9unJ_zqb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyYK--AdBcrT"
      },
      "source": [
        "## Vector space model\n",
        "\n",
        "* Algorithms for information retrieval\n",
        "* Research began around 1970\n",
        " * The SMART system of Dr. Salton and others is famous.\n",
        "\n",
        "* Represent the search target data and search words as vectors and place them in the vector space.\n",
        "* Calculate the similarity between search target data and search terms using vector calculations (cosine, inner product, distance, etc.).\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1Pj7FI_hkdMK-jlpqdmNkZvPRHQRWbJ01' width='60%'>\n",
        "</center>\n",
        "\n",
        "<center>Figure 11. Vector space model </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prbhDcajopBJ"
      },
      "source": [
        "## Cosine similarity\n",
        "The cosine measure is used as a measure of vector closeness. The proximity of vectors calculated by cosine measure is called \"cosine similarity\". Suppose a query vector and a data vector be $\\mathbf{q}, \\mathbf{d}$, respectively, cosine similarity between $\\mathbf{q}$ and $\\mathbf{d}$ is calculated by the formula (10). Here, a vector variable (e.g. $\\mathbf{q}$ and $\\mathbf{d}$) is described in bold.\n",
        "\n",
        "$$ C(q, d) = \\frac{\\mathbf{q}\\cdot \\mathbf{d}}{|\\mathbf{q}||\\mathbf{d}|} \\tag{10}$$\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1T_iVHLGLR3tXiFXdU741klw8bCdDnUbr' width='50%'>\n",
        "</center>\n",
        "<center>Figure 14. Cosine measure </center>\n",
        "\n",
        "Suppse that $\\mathbf{q} = (1, 0, 1), \\mathbf{d_1} = (1, 1, 1), \\mathbf{d_2} = (0, 1, 1)$,cosine similarities $C(\\mathbf{q}, \\mathbf{d_1})$ for $\\mathbf{q}$ and $\\mathbf{d_1}$ and $C(\\mathbf{q}, \\mathbf{d_2})$ for $\\mathbf{q}$ and $\\mathbf{d_2}$ are caluculated as follows.\n",
        "\n",
        "$$ C(\\mathbf{q}, \\mathbf{d_1}) = \\frac{(1,0,1)\\cdot (1,1,1)}{|(1,0,1)||(1,1,1)|} = \\frac{1\\times 1 + 0\\times 1 + 1\\times 1}{\\sqrt{1^2+0^2+1^2 }\\sqrt{1^2+1^2+1^2}} = \\frac{2}{1.41\\times 1.73} = 0.81$$\n",
        "$$ C(\\mathbf{q}, \\mathbf{d_2}) = \\frac{(1,0,1)\\cdot (0,1,1)}{|(1,0,1)||(0,1,1)|} = \\frac{1\\times 0 + 0\\times 1 + 1\\times 1}{\\sqrt{1^2+0^2+1^2 }\\sqrt{0^2+1^2+1^2}} = \\frac{1}{1.41\\times 1.41} = 0.50$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjnKL_6Votxg"
      },
      "source": [
        "### Code example\n",
        "\n",
        "Let's create a function cosine_sim() that calculates the cosine similarity of the embedded vector, and try to calculate the cosine similarity between similarity the embedded vectors of \"car\" and \"truck\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00ojpNAlmJTq"
      },
      "source": [
        "# Calculate cosine similarity\n",
        "def cosine_sim(w1, w2):\n",
        "  cosine_value = np.dot(model.wv[w1], model.wv[w2]) / (np.linalg.norm(model.wv[w1]) * np.linalg.norm(model.wv[w2]))\n",
        "\n",
        "  return cosine_value\n",
        "\n",
        "print (cosine_sim('car','truck'))\n",
        "\n",
        "# ('truck', 0.7117820978164673)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il7CwmCEpzxf"
      },
      "source": [
        "The cosine similarity is output as 0.7151368, and you can see that the same value is calculated as the result of \"model.wv.most_similar(['car'])\" earlier. Here, please note that the values ​​may differ depending on the individual environment.\n",
        "\n",
        "[('driver', 0.7645907402038574), <br>\n",
        " ('cars', 0.7256535291671753), <br>\n",
        " ('motorcycle', 0.7231885194778442), <br>\n",
        " ('taxi', 0.7163602113723755), <br>\n",
        " ('truck', 0.7151368856430054), <br>\n",
        " ('vehicle', 0.6943105459213257), <br>\n",
        "...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMPoB4b-DmNA"
      },
      "source": [
        "### **Practice 2-5**\n",
        "Let's use the function cosine_sim() to calculate the cosine smilarity of the embedded vectors of \"car\" and \"train.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzhr6xuYv5u4"
      },
      "source": [
        "## Document-term matrix\n",
        "* A matrix characterized by a set of terms ($t_1$ to $t_n$) that appear in a document ($d_1$ to $d_m$)\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1ErQ36FOlADuk54RF2qvPih4OTCS-0qXN' width='40%'>\n",
        "</center>\n",
        "<center>Figure 12. Document-term matrix </center>\n",
        "<br>\n",
        "\n",
        "* Example of creating document-term matrix\n",
        "\n",
        "There are five documents $d_1, d_2, d_3, d_4, d_5$ that include five words.\n",
        "\n",
        "Step-1: Extract all words<br>\n",
        "In the example, from five documents, a set of words, apple, banana, grape, pear, strawberry, mango, melon, watermelon, peach, tangerine, mathematics, physics, French, geography, chemistry, Japanese, English, are extracted.\n",
        "\n",
        "Step-2: Create a document-term matrix<br>\n",
        "We make a document-term matrix, setting words on the horizontal axis and documents on the vertical axis.\n",
        "\n",
        "Step-3: Add feature values<br>\n",
        "For each document, add \"1\" if the word appears in the corresponding document, otherwise add \"0\".\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=171homWzJFytRIsIaRSCLm_75Lu0N1dLU' width='70%'>\n",
        "</center>\n",
        "<center>Figure 13. Basic creation flow of document-term matrix </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creation of document vector\n",
        "In Figure 13, doc1 is represented as a vector $\\mathbf{d}_1 = (1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ,0)$\n",
        "\n",
        "Smilarly, one-hot vectors of terms 'apple', 'banana', 'grape', 'paer', 'strawberry' are represented as follows.\n",
        "\n",
        "$\\mathbf{t}_{apple} = (1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)$<br>\n",
        "$\\mathbf{t}_{banana} = (0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)$<br>\n",
        "$\\mathbf{t}_{grape} = (0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0)$<br>\n",
        "$\\mathbf{t}_{paer} = (0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0)$<br>\n",
        "$\\mathbf{t}_{strawberry} = (0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)$<br>\n",
        "\n",
        "In this case, doc1 is considered as the addition of vectors of terms appeared in doc1.\n",
        "\n",
        "$\\mathbf{d}_1 = \\mathbf{t}_{apple}+ \\mathbf{t}_{banana}+ \\mathbf{t}_{grape}+ \\mathbf{t}_{paer}+ \\mathbf{t}_{strawberry}$  "
      ],
      "metadata": {
        "id": "-fkoPFUOJfGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code example\n",
        "Let'S create a document vector by vectorizing the words in the document and adding those word vectors.Here, we use term embedded  vectors instead of one-hot vectors.\n",
        "\n",
        "document 1: apple, banana, grape, pear, strawberry\n",
        "\n",
        "document 2: grape, pear, mango, melon, watermelon"
      ],
      "metadata": {
        "id": "4IrKROEPeNOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vec1 = model.wv['apple'] + model.wv['banana'] + model.wv['grape'] + model.wv['pear'] + model.wv['strawberry']\n",
        "\n",
        "print (doc_vec1)"
      ],
      "metadata": {
        "id": "yyw5TIVaJcU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vec2 = model.wv['grape'] + model.wv['pear'] + model.wv['mango'] + model.wv['melon'] + model.wv['watermelon']\n",
        "\n",
        "print (doc_vec2)"
      ],
      "metadata": {
        "id": "_k9RcwJyK-l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculates the cosine similarity of document 1 and document 2.\n",
        "\n"
      ],
      "metadata": {
        "id": "pJNcynqiLKl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " cosine_value = np.dot(doc_vec1, doc_vec2) / (np.linalg.norm(doc_vec1) * np.linalg.norm(doc_vec2))\n",
        "\n",
        " print(cosine_value)"
      ],
      "metadata": {
        "id": "Um32S5NVLOPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 2-6**\n",
        "Let's modify the cosine_sim function and create a function cosine_sim_vec(v1, v2) that takes two vectors as input and returns the cosine similarity."
      ],
      "metadata": {
        "id": "7nGKQu1FMNX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 2-7**\n",
        "The following equations shows that the semantic calculation is possible by the vector operations for the word embedded vectors.\n",
        "\n",
        "vector('Paris') - vector('France') + vector('Italy') = vector('Rome')\n",
        "\n",
        "vector('king') - vector('man') + vector('woman') = vector('queen')\n",
        "\n",
        "<br>\n",
        "1. Using embedded vectors by word2vec, create document vectors for right side and the left side of the above equations, respectively.<br>\n",
        "2. Using the function cosine_sim(), calculate the cosine similarity. <br>\n",
        "3. Based on the results of (2), check if the equations are correct or not.\n",
        "\n"
      ],
      "metadata": {
        "id": "t_TJCLUwBI4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 2-8**\n",
        "\n",
        "There are five documents  d3,d4,d5  that include five words as follows.\n",
        "\n",
        "d3: peach, melon, banana, strawberry, orange<br>\n",
        "d4: mathematics, physics, french, geography, chemistry<br>\n",
        "d5: mathematics, chemistry, japanese, geography, english<br>\n",
        "\n",
        "1.   Create each document vector for d3, d4, d5.\n",
        "2.   Calculate the cosine similarity of documents 3 and 4, and the cosine similarity of documents 4 and 5, respectively.\n",
        "3.   Explain the similarity between documents 3 and 4 (how similar they are) and the similarity between documents 4 and 5 based on the content of the documents and the values of the cosine similarity.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dPdfaj7_JXgc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DnM6Zkwy33I"
      },
      "source": [
        "## References\n",
        "* https://code.google.com/archive/p/word2vec/\n",
        "* Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013.\n",
        "* François Chollet, Deep Learning with Python"
      ]
    }
  ]
}