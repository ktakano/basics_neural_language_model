{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfKkr0Hbin3I"
      },
      "source": [
        "# Short lecture on \"Basics of Neural Language Model\"\n",
        "\n",
        "**Lecturer: Prof. Kosuke Takano, Kanagawa Institute of Technology**\n",
        "\n",
        "This short lecture instructs the basics of neural language model along with simple python codes. The Large Language Model (LLM) such as OpenAI's ChatGPT and Goolge's Gemini are dramatically changing our life and society with their awesome human-like capability, however their mechanism is not so complicated. This lecture aims to focus on basic components to build the LLM and enlighten how they work in a neural network architecture. Student will write small codes of basic functions consisting of neural networks for the natural language processing and deepen the understanding on the principle."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "Day 1:\n",
        "* Basic of neural network\n",
        "* Word embedding\n",
        "* Sequential neural model for Natural Language Processing\n",
        "\n",
        "Day 2:\n",
        "* Sequential neural model for Natural Language Processing (Cont.)\n",
        "* Transformer\n",
        "* Conversation application by GPT"
      ],
      "metadata": {
        "id": "omjm33Mmusry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirement\n",
        "* PC and Internet connection\n",
        "* Google Colaboratory ... Google account is required"
      ],
      "metadata": {
        "id": "eRq8iy9Euuc_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrkZlKGLojT-"
      },
      "source": [
        "## Execution environment\n",
        "\n",
        "Python programs are very version sensitive.Since the execution environment of Colaboratory will be updated at google's discretion, so we need to check it.<br>\n",
        "Python: 3.10.12 (Februrary 27, 2024)<br>\n",
        "TensorFlow: 2.15.0 (Februrary 27, 2024\n",
        "\n",
        "Be sure to specify GPU or TPU as the runtime type."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYiJ7Ou2BETI",
        "outputId": "e5529c5c-5141-48a3-ecd3-b28db4edd44f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOtGkP4UBBkw",
        "outputId": "f1de1101-eb1c-4ee5-db74-50d67ff6e1c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3"
      ],
      "metadata": {
        "id": "qmOf4BErwlNT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5YzHj636Vhb"
      },
      "source": [
        "## RNN (Recurrent Neural Network)\n",
        "\n",
        "* Neural network with recurrent architecture\n",
        "* Used in fields such as natural language processing.\n",
        " * Document classification, sentiment analysis, machine translation, text generation\n",
        "* There are derivative models such as LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit).\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1WkAbRVWHZd6aA1-X9WeqO75ojjiQQihk' width='70%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 1. RNN architecture\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIADM_yMfh0V"
      },
      "source": [
        "## Detail of RNN cell\n",
        "In Figure 1, the part marked \"*$H$*\" will be called the RNN layer. Each cell $h_t$ in $H$ is called RNN cell, and the output is recullently calculated using the output in prevous RNN cell. Figure 2 shows the architecture of the RNN cell. As shown in Figure 2, the relationship between the input $\\mathbf{x}_t$ and output $\\mathbf{o}_t$ in the RNN cell $h_t$ is calculated by the following formula.<br><br>\n",
        "\n",
        "$$\\mathbf{o}_t = tanh(\\mathbf{o}_{t-1} \\mathbf{W}_o + \\mathbf{x}_t \\mathbf{W}_x  + \\mathbf{b}) \\tag{1}$$\n",
        "<br>\n",
        "\n",
        "* $\\mathbf{x}_t$ is a term vector\n",
        "* $\\mathbf{o}_{t-1}$ is an output from the previous layer\n",
        "* $\\mathbf{W}_x$ for $\\mathbf{x}_t$ is the weighting coefficient\n",
        "* $\\mathbf{W}_o$ for $\\mathbf{o}_{t-1}$ is the weighting coefficient\n",
        "* $\\mathbf{b}$ is the bias value\n",
        "* $tanh$ is a hyperbolic tangent function, which is used as an activation function.\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=105u8Lk7z7RkSfgDTAc_DMgGIpGWbid2q' width='70%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 2. RNN cell architecture\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic calculation of RNN cell\n",
        "Figure 3 shows a basic calculation of RNN cell. RNN cell $h_{t}$ returns one output $\\mathbf{o}_t$ vector for two input vectors $\\mathbf{x}_t$ and $\\mathbf{o}_{t-1}$. Here, $\\mathbf{x}_t$ is a term vector and $\\mathbf{o}_{t-1}$ is a vector of the output from previous RNN cell $h_{t-1}$. The calculation steps are as follows.\n",
        "\n",
        "Step-1: Calculate the output of previous RNN cell $h_{t-1}$ and input it into $h_{t}$<br>\n",
        "Step-2: Input a term vector $\\mathbf{x}_t$ into  $h_{t}$<br>\n",
        "Step-3: Multiply $\\mathbf{o}_{t-1}$ with the weighting cofficient $\\mathbf{W}_o$<br>\n",
        "Step-4: Multiply $\\mathbf{x}_{t}$ with the weighting cofficient $\\mathbf{W}_x$\n",
        "Step-5: Add bias $\\mathbf{b}$\n",
        "Step-6: Finally, tanh function is applied and the $\\mathbf{o}_{t}$ is output\n",
        "\n",
        "In Step-3, as shown in Figure 3, if $\\mathbf{x}_{t}$ is a 9 dimensions vector and $\\mathbf{W}_x$ is a 9 x 3 matrix, a 3 dimensions vector is output by calculation.\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1UtAIUFPtClrCQpkXY86azqNzG8yFW4TF' width='65%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 3. Calculation of RNN cell\n",
        "</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "HSzmjtt0CedA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code example\n",
        "\n",
        "Consider the leftmost cell. The leftmost cell has no $o_{t-1}$ input, so the formula is as follows.Here, a word vector is input as $\\mathbf{x}_1$.\n",
        "\n",
        "$$ \\mathbf{o}_1 = tanh(\\mathbf{x}_1 \\mathbf{W}_x + \\mathbf{b}) \\tag{3}$$\n",
        "\n",
        "Let's write a code for the above calculation formula with the number of dimensions of the word vector as 100 and the number of dimensions inside the cell (the number of dimensions of the hidden layer) as 5."
      ],
      "metadata": {
        "id": "CA-en-3VDkWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "wordvec_size = 100\n",
        "hidden_size = 5\n",
        "\n",
        "Wx = np.random.randn(wordvec_size, hidden_size)\n",
        "b = np.zeros(hidden_size)\n",
        "print(Wx.shape)\n",
        "print(b.shape)"
      ],
      "metadata": {
        "id": "aEi3xNNVCd9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use word vectors of an embedding matrix trained by word2vec. Run the previous code and generate a word embedding matrix."
      ],
      "metadata": {
        "id": "6QLvSn3lFV3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://mattmahoney.net/dc/text8.zip"
      ],
      "metadata": {
        "id": "bX2anqhrEy-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip text8.zip"
      ],
      "metadata": {
        "id": "RSJeNikAE1eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from gensim.models.word2vec import Word2Vec, Text8Corpus\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "sentences = Text8Corpus('text8')\n",
        "model = Word2Vec(sentences, vector_size=100)\n",
        "\n",
        "model.save('model.bin')"
      ],
      "metadata": {
        "id": "oXpmJEVaE6j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec.load('model.bin')\n",
        "\n",
        "x = model.wv['dog']"
      ],
      "metadata": {
        "id": "jH4fkLeJE-ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "id": "wTqU_4xknPuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word embedding vector of \"dog\" is obtained. Let's do the calculation above."
      ],
      "metadata": {
        "id": "Ku-xd3HWGTZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_o = np.dot(x, Wx) + b\n",
        "print(_o)"
      ],
      "metadata": {
        "id": "DkwEwDRoGN6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, input it to the activation function tanh()."
      ],
      "metadata": {
        "id": "bbhI90dZKAyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "o = np.tanh(_o)\n",
        "print(o)"
      ],
      "metadata": {
        "id": "NHXsGah-KG7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practice 3-1\n",
        "1. Let's create a function RNN_cell0(x, Wx, b) that calculates the leftmost RNN cell.\n",
        "2. Calculate the output of RNN_cell0(x, Wx, b) for the word of \"cat\"."
      ],
      "metadata": {
        "id": "d-uOw8S7GrUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RNN_cell0(x, Wx, b):\n",
        "  _o = np.dot(x, Wx) + b\n",
        "  o = np.tanh(_o)\n",
        "\n",
        "  return o"
      ],
      "metadata": {
        "id": "LUcwB3jjox3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = model.wv['cat']\n",
        "o = RNN_cell0(x, Wx, b)\n",
        "\n",
        "print(o)"
      ],
      "metadata": {
        "id": "1IUpIHFVWV4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code example\n",
        "\n",
        "In the second and subsequent RNN cells, $\\mathbf{o}_{t-1}$, which is the output of the previous RNN cell, is also input and calculated. The calculation formula for the second RNN cell is as follows.\n",
        "\n",
        "$$ \\mathbf{o}_2 = tanh(\\mathbf{o}_1 \\mathbf{W}_o + \\mathbf{x}_2 \\mathbf{W}_x +\\mathbf{b}) \\tag{4}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "JGsJPWRgHRsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's calculate the first and second RNN cells for the sentence \"I like dog\"."
      ],
      "metadata": {
        "id": "Yr_qrE4rIFdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = model.wv['i']\n",
        "x2 = model.wv['like']"
      ],
      "metadata": {
        "id": "NBWTI9tlIDww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the weight matrix and bias in the same way as before. $\\mathbf{W}_{o}$ is a new addition."
      ],
      "metadata": {
        "id": "FCGPbKZVIuTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordvec_size = 100\n",
        "hidden_size = 5\n",
        "\n",
        "Wx = np.random.randn(wordvec_size, hidden_size)\n",
        "Wo = np.random.randn(hidden_size, hidden_size)\n",
        "b = np.zeros(hidden_size)"
      ],
      "metadata": {
        "id": "UtGFHZafImcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_o1 = np.dot(x1, Wx) + b\n",
        "o1 = np.tanh(_o1)\n",
        "\n",
        "_o2 = np.dot(o1, Wo) + np.dot(x2, Wx) + b\n",
        "o2 = np.tanh(_o2)\n",
        "\n",
        "print(o1)\n",
        "print(o2)"
      ],
      "metadata": {
        "id": "b2nSa4NLI9Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practice 3-2\n",
        "Calculate the output of the third RNN cell for the sentence 'I like dog'."
      ],
      "metadata": {
        "id": "EUnLjNsCJMRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x3 = model.wv['dog']\n",
        "_o3 = np.dot(o2, Wo) + np.dot(x3, Wx) + b\n",
        "o3 = np.tanh(_o3)\n",
        "\n",
        "print(o3)"
      ],
      "metadata": {
        "id": "k3TgKBmGrTw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 3-3**\n",
        "1. Create a function RNN_cell(x, o, Wx Wo, b) that calculates the second and subsequent RNN cells.\n",
        "2. Calculate the output of the third RNN cell for the sentence \"You look mirror\"."
      ],
      "metadata": {
        "id": "IaoGH_t-JUxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RNN_cell(x, o, Wx, Wo, b):\n",
        "\n",
        "  _o = np.dot(o, Wo) + np.dot(x, Wx) + b\n",
        "  o = np.tanh(_o)\n",
        "\n",
        "  return o"
      ],
      "metadata": {
        "id": "OZQ3dJwlrjWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = model.wv['you']\n",
        "x2 = model.wv['look']\n",
        "x3 = model.wv['mirror']\n",
        "\n",
        "o1 = RNN_cell0(x1, Wx, b)\n",
        "o2 = RNN_cell(x2, o1, Wx, Wo, b)\n",
        "o3 = RNN_cell(x3, o2, Wx, Wo, b)\n",
        "\n",
        "print(o1)\n",
        "print(o2)\n",
        "print(o3)"
      ],
      "metadata": {
        "id": "7sUMhShxj-N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part-4"
      ],
      "metadata": {
        "id": "awuoDUjXqwU2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7K7RT43uv1v"
      },
      "source": [
        "## Sentiment analysis by RNN\n",
        "\n",
        "\n",
        "Sentiment analysis and document classification are applied natural language processing tasks. Sentiment analysis is a task for prediting emotion for the input sentence. Document classification task requires to classify documents according to their content.\n",
        "When building a neural network for natural language processing tasks, a word embedding layer is often used for the input layer.\n",
        "\n",
        "Figure 4 shows an example architecture of RNN for postive-negative judgement on input sentece.Sentence is split into $n$ words,$t_1, t_2, \\cdots, t_n$, and each splited word is input into each RNN cell recurrently.\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1RN08BTtS54Ubgk0QA1qLNFBK_ixwWchF' width='50%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 4. Positive - negative judgment by RNN\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMDb dataset\n",
        "* The IMDb dataset (`http://ai.stanford.edu/~amaas/data/sentiment/`) is a collection of 50,000 movie reviews, with 25,000 positive and negative (25,000) reviews. Consists of 25,000 negative (negarive) ratings. Here, movie reviews are text data. Positive and negative evaluations are assigned to each sentence as correct answer."
      ],
      "metadata": {
        "id": "BqL-NXyBm345"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code example\n",
        "\n",
        "Let's classify movie reviews into positive and negative for the IMDB dataset. Here, we will use the IMDB data included with keras. There are 25,000 pieces of training data (x_train, y_train) and 25,000 pieces of test data (x_test, y_test). Also, y_train (y_test) stores postive (= 1) or negative (= 0)."
      ],
      "metadata": {
        "id": "OZ_ZGXsSgTGw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nOtjxJ-6zUn"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "max_words = 10000  # Number of terms\n",
        "max_len = 500  # Length of input sequence\n",
        "embedding_dim = 128 # Dimension of word embedding\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) =  tf.keras.datasets.imdb.load_data(num_words=max_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We check both train data and test data.Train data cosists of consisting of body data (x_train) and answers (y_train) and is used for training a model, In IMdB, body data (x_train) is a list of word ids proprocessed from the original review sentence. Answer (y_train) is answer for the review sentence, 1 or 0. Test data (x_test, y_test) is used to test the trained model and consists of data set with the same structure as training data.  "
      ],
      "metadata": {
        "id": "dplJWJ7QvCxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train), 'train (sequence)')\n",
        "print(len(x_test), 'test (sequence)')"
      ],
      "metadata": {
        "id": "DQoMuLNCsBqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train[0]))\n",
        "print(x_train[0])\n",
        "\n",
        "print(len(x_train[1]))\n",
        "print(x_train[1])"
      ],
      "metadata": {
        "id": "prbamR19sorU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 4-1**\n",
        "\n",
        "1. Check the number of data in y_train and y_test.\n",
        "2. Display the values ​​of y_train[0] and y_train[1]."
      ],
      "metadata": {
        "id": "rBiwat9kwy5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code example**\n",
        "\n",
        "Using index information, each word id can be converted to the correspond word."
      ],
      "metadata": {
        "id": "wxEQRZW7v8aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2id =  tf.keras.datasets.imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "\n",
        "print(len(word2id))\n",
        "print(len(id2word))"
      ],
      "metadata": {
        "id": "gG_iZSswtRP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Index number of 'car':\")\n",
        "print(word2id['car'])\n",
        "\n",
        "print('Words review sentence:')\n",
        "print([id2word.get(i, ' ') for i in x_train[0]])"
      ],
      "metadata": {
        "id": "yBI-Kg_5tViS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code example**\n",
        "\n",
        "**Padding**\n",
        "\n",
        "The word count (document length) of review data x_train[0] and x_train[1] was 218 words and 189 words, respectively. When training a neural network, input the same number of words. The process of filling in spaces with characters (such as 0) to equalize the number of words is called \"padding\"."
      ],
      "metadata": {
        "id": "5WBl6-pWsU_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding\n",
        "\n",
        "max_len = 500\n",
        "x_train = tf.keras.utils.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = tf.keras.utils.pad_sequences(x_test, maxlen=max_len)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "metadata": {
        "id": "0RnoNfPsLcmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train[0]))\n",
        "print(x_train[0])"
      ],
      "metadata": {
        "id": "BBt3S0oNLm3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 4-2**\n",
        "\n",
        "1. Display x_train[1] and x_train[2].\n",
        "2. Similarly, apply padding to x_test to a length of 500."
      ],
      "metadata": {
        "id": "UCOnG8t85gq_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjHgkhTzvkUw"
      },
      "source": [
        "## Applying RNN model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code example\n",
        "\n",
        "* Build an RNN model using SimpleRNN.\n",
        "* Embedding() is a word embedding layer that vectorizes words."
      ],
      "metadata": {
        "id": "7UOWU1Vw6lEp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bO5CiS28D5O"
      },
      "source": [
        "from keras.layers import SimpleRNN, Embedding, Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "embedding_dim=128\n",
        "model_rnn=Sequential()\n",
        "model_rnn.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
        "model_rnn.add(SimpleRNN(100))\n",
        "model_rnn.add(Dense(1, activation='sigmoid'))\n",
        "print(model_rnn.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdOj2aK99zur"
      },
      "source": [
        "model_rnn.compile(loss='binary_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['acc'])\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 3\n",
        "\n",
        "history = model_rnn.fit(x_train, y_train,\n",
        "              validation_split=0.2,\n",
        "              batch_size=batch_size, epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWxyCxidgN3r"
      },
      "source": [
        "We evaluate the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zIyhYR9ythO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhCThjUh-FdZ"
      },
      "source": [
        "scores = model_rnn.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_test[0].shape)\n",
        "print(x_test[0][np.newaxis].shape)"
      ],
      "metadata": {
        "id": "fA-e_cOuva4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred0 = model_rnn.predict(x_test[0][np.newaxis])\n",
        "\n",
        "print('Predition: ', pred0)\n",
        "print('Answer: ', y_test[0])"
      ],
      "metadata": {
        "id": "kmG0bEmcRSzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 4-3**\n",
        "Similarly, check the prediction result is correct for x_test[1] and x_test[2]."
      ],
      "metadata": {
        "id": "kJxOBrHv-H5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred1 = model_rnn.predict(x_test[1][np.newaxis])\n",
        "\n",
        "print('Prediction: ', pred1)\n",
        "print('Answer: ', y_test[1])\n",
        "\n",
        "pred2 = model_rnn.predict(x_test[2][np.newaxis])\n",
        "\n",
        "print('Prediction: ', pred2)\n",
        "print('Answer: ', y_test[2])"
      ],
      "metadata": {
        "id": "VTpFzkdQwILN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred0_2 = model_rnn.predict(x_test[0:3])\n",
        "\n",
        "print(pred0_2)\n",
        "\n",
        "print(y_test[0:3])"
      ],
      "metadata": {
        "id": "Pgz9t-bcwZdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 4-4**\n",
        "* Increase the number of epochs to 10 in the RNN model and run it. Let's also draw classification accuracy and loss graphs. (If it takes longer to execute, you can reduce the number of epochs.)\n",
        "* Predict x_test[0], x_test[1], x_test[2] and see if there is a change in the results compared to when the number of epochs is small."
      ],
      "metadata": {
        "id": "ZjhTPoucQ9MP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kh_QDcXMtmO"
      },
      "source": [
        "## LSTM (Long short-term memory )\n",
        "* It is a derivative type of neural network called RNN.\n",
        "* RNNs have the problem of not being able to learn well because gradient vanishing occurs during learning.\n",
        "* LSTM prevents the problem of vanishing gradients during learning by adding a separate memory cell to transfer memory to the next LSTM layer.\n",
        "\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1FtxOh6SmN56d8cDyuAFwKN6dtl5AO-4C' width='70%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 5. LSTM cell architecture\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL7_mX_s07If"
      },
      "source": [
        "### Applying LSTM model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code example**\n",
        "* Build a prediction model using LSTM.\n",
        "* You can build the model you created earlier by simply replacing SimpleRNN with LSTM."
      ],
      "metadata": {
        "id": "GUKkO2xm--Dq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn_MA3r569Tc"
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "embedding_dim=128\n",
        "model_lstm=Sequential()\n",
        "model_lstm.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
        "model_lstm.add(LSTM(100))\n",
        "model_lstm.add(Dense(1, activation='sigmoid'))\n",
        "print(model_lstm.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0JsU5S17Gqj"
      },
      "source": [
        "model_lstm.compile(loss='binary_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['acc'])\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 5\n",
        "\n",
        "history = model_lstm.fit(x_train, y_train,\n",
        "              validation_split=0.2,\n",
        "              batch_size=batch_size, epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRPJnwYl4Is0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIClYP847NZc"
      },
      "source": [
        "scores = model_lstm.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 4-5**\n",
        "* Increase the number of epochs to 10 in the LSTM model and run it. Let's also draw classification accuracy and loss graphs. (If it takes longer to execute, you can reduce the number of epochs.)\n",
        "* Predict x_test[0], x_test[1], x_test[2] and see if there is a change in the results compared to when the number of epochs is small."
      ],
      "metadata": {
        "id": "OIEgTI4vR0rs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4kulnLA8GUP"
      },
      "source": [
        "### GRU\n",
        "* Similar to LSTM, it is a derivative type of neural network for RNN.\n",
        "*This model has fewer parameters than LSTM and has lower calculation costs.\n",
        "* Unlike LSTM, it does not use memory cells, but it is structured that gradient vanishing does not occur during learning.\n",
        "\n",
        "<br>\n",
        "<img src='https://drive.google.com/uc?export=view&id=13TskGR7UE5Mm2E5ofvOKxGIXl2xQotfQ' width='90%'>\n",
        "</center>\n",
        "<center>\n",
        "Figure 6. Comaprison of cell architectures of RNN, LSTM, GRU\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc5waCVp8cOv"
      },
      "source": [
        "from keras.layers import GRU\n",
        "embedding_dim=128\n",
        "model_gru=Sequential()\n",
        "model_gru.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
        "model_gru.add(GRU(100))\n",
        "model_gru.add(Dense(1, activation='sigmoid'))\n",
        "print(model_gru.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFPPmOEP96t0"
      },
      "source": [
        "model_gru.compile(loss='binary_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['acc'])\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 5\n",
        "\n",
        "history = model_gru.fit(x_train, y_train,\n",
        "              validation_split=0.2,\n",
        "              batch_size=batch_size, epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTcdRLpv4hz6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXE_rLsK-JwK"
      },
      "source": [
        "scores = model_gru.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Practice 4-6**\n",
        "* Increase the number of epochs to 10 in the GRU model and run it. Let's also draw classification accuracy and loss graphs. (If it takes longer to execute, you can reduce the number of epochs.)\n",
        "* Predict x_test[0], x_test[1], x_test[2] and see if there is a change in the results compared to when the number of epochs is small."
      ],
      "metadata": {
        "id": "qbtglZmgR-DP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g32B24np9YgD"
      },
      "source": [
        "### **Practice 4-7**\n",
        "* Compare and discuss the classification accuracy of the three models: SimpleRNN, LSTM, and GRU.\n",
        "* Furthermore, let's compare by also focusing on the number of model parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFnnFH5MfnXC"
      },
      "source": [
        "## References\n",
        "* https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms\n",
        "* Keras official Website, https://keras.io/examples/"
      ]
    }
  ]
}